{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243f5b05",
   "metadata": {},
   "source": [
    "# Cusp-capturing PINN - Example 4\n",
    "\n",
    "* level set augmentation: $\\phi_a = |\\phi|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d1b38f",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b603d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "from pyDOE import lhs\n",
    "\n",
    "from functorch import make_functional, vmap, grad, jacrev, hessian\n",
    "\n",
    "from collections import namedtuple, OrderedDict\n",
    "import datetime\n",
    "import time\n",
    "from IPython.display import clear_output \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d37a549",
   "metadata": {},
   "source": [
    "### Empty cache and check devices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f40cfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device =', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85db90d",
   "metadata": {},
   "source": [
    "### Pre-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b578d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain [a,b]\\times[c,d]\\times[e,f]\n",
    "bda = - 1.\n",
    "bdb = 1.\n",
    "bdc = - 1.\n",
    "bdd = 1.\n",
    "bde = - 1.\n",
    "bdf = 1.\n",
    "# btai, btao, r0, and (0.5r^4_0+r^2_0)/b\n",
    "btao = 1000\n",
    "btai = 1. + 0.25\n",
    "r0sq = (0.5)**2\n",
    "cexu = r0sq - ( 1. + 0.5*r0sq )*(r0sq/btao)\n",
    "# beta at interface\n",
    "bta_jmp = btao - btai\n",
    "bta_avg = 0.5*( btao + btai )\n",
    "# radii of the ellipse\n",
    "rad = .5\n",
    "rsq = rad * rad\n",
    "# Network size\n",
    "n_input  = 4\n",
    "n_hidden = 9\n",
    "n_output = 1\n",
    "n_depth = 3 # only used in deep NN\n",
    "# tolerence for LM\n",
    "tol_main    = 10**(-10)\n",
    "tol_machine = 10**(-15)\n",
    "mu_max      = 10**8\n",
    "mu_ini      = 10**8\n",
    "# iteration counts and check\n",
    "tr_iter_max    = 3000                      # max. iteration\n",
    "ts_input_new   = 500                       # renew testing points \n",
    "ls_check       = 500\n",
    "ls_check0      = ls_check - 1\n",
    "# number of training points and testing points\n",
    "c_addpt = 1\n",
    "N_trd = 800\n",
    "N_trb = 20\n",
    "N_trg = 160\n",
    "N_tsd_final = 1000*N_trd\n",
    "N_tsg_final = 1000*N_trg\n",
    "dxh = 0.5 * ( bdb - bda ) / N_trb\n",
    "dyh = 0.5 * ( bdd - bdc ) / N_trb\n",
    "dzh = 0.5 * ( bdf - bde ) / N_trb\n",
    "# filename for training and testing points on the interface gamma\n",
    "fname_gma_tr = 'X_gma_N23230_rx0p5ry0p5rz0p5.txt'\n",
    "fname = 'ex3DcompareIIM_b1000y_D9L3M3360'\n",
    "char_id = 'a' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7c8f23",
   "metadata": {},
   "source": [
    "### Exact solution, level set function, and right-hand-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "878b481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levfun(opt, x, y, z):\n",
    "    qo = ( x*x + y*y + z*z )/rsq - 1.0\n",
    "    if opt == 0:\n",
    "        q = np.zeros_like(x)\n",
    "        for i in range(len(qo)):\n",
    "            if qo[i] >= 0.:\n",
    "                q[i] = qo[i]\n",
    "            else:\n",
    "                q[i] = - qo[i]\n",
    "    else:\n",
    "        q = np.zeros( (len(x),6) )\n",
    "        q[:,0:1] = qo\n",
    "        q[:,1:2] = 2.*x/rsq\n",
    "        q[:,2:3] = 2.*y/rsq\n",
    "        q[:,3:4] = 2.*z/rsq\n",
    "        q[:,5:6] = 6./rsq\n",
    "        for i in range(len(x)):\n",
    "            if qo[i] >= 0.:\n",
    "                q[i,:] = q[i,:]\n",
    "            else:\n",
    "                q[i,:] = - q[i,:]\n",
    "        q[:,4:5] = q[:,1:2] * q[:,1:2] + q[:,2:3] * q[:,2:3] + q[:,3:4] * q[:,3:4]\n",
    "    return q, qo\n",
    "\n",
    "\n",
    "def lvnorvec(x, y, z):\n",
    "    qnor = np.zeros( (len(x),3) )\n",
    "    dqx = x/rsq\n",
    "    dqy = y/rsq\n",
    "    dqz = z/rsq\n",
    "    dqsqrt = np.sqrt( dqx*dqx + dqy*dqy + dqz*dqz )\n",
    "    qnor[:,0:1] = dqx/dqsqrt\n",
    "    qnor[:,1:2] = dqy/dqsqrt\n",
    "    qnor[:,2:3] = dqz/dqsqrt\n",
    "    return qnor\n",
    "\n",
    "\n",
    "# exact solution \n",
    "def exact_u(x, y, z, q):\n",
    "    eu = np.zeros_like(q)\n",
    "    dsq = x*x + y*y + z*z\n",
    "    for i in range(len(q)):\n",
    "        if q[i] >= 0.:\n",
    "            eu[i] = ( 1. + 0.5*dsq[i] )*(dsq[i]/btao) + cexu\n",
    "        else:\n",
    "            eu[i] = dsq[i]\n",
    "    return eu\n",
    "\n",
    "\n",
    "# normal derivative jump of level set function\n",
    "def jump_lvdn(x, y, z):\n",
    "    qdn_jmp = (4/rsq) * np.sqrt( x*x + y*y + z*z )\n",
    "    return qdn_jmp\n",
    "\n",
    "\n",
    "def btafun(x, y, z, q):\n",
    "    dsq = x*x + y*y + z*z\n",
    "    bta_d = np.zeros( (len(q),4) )\n",
    "    for i in range(len(q)):\n",
    "        if q[i] >= 0.:\n",
    "            bta_d[i,0] = btao\n",
    "            bta_d[i,1] = 0.\n",
    "            bta_d[i,2] = 0.\n",
    "            bta_d[i,3] = 0.\n",
    "        else:\n",
    "            bta_d[i,0] = dsq[i] + 1\n",
    "            bta_d[i,1] = 2*x[i]\n",
    "            bta_d[i,2] = 2*y[i]\n",
    "            bta_d[i,3] = 2*z[i]\n",
    "    return bta_d\n",
    "\n",
    "\n",
    "# normal derivative jump condition along interface: [\\beta\\partial_n u]=0 \n",
    "def jump_btadun(x, y, z):\n",
    "    dun = np.zeros_like(x)\n",
    "    return dun\n",
    "\n",
    "\n",
    "# source\n",
    "def lapu(x, y, z, q):\n",
    "    dsq = x*x + y*y + z*z\n",
    "    ef = 10*dsq + 6.            \n",
    "    return ef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c37cfb2",
   "metadata": {},
   "source": [
    "### Define networks: Shallow and Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "897286f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet_Shallow(torch.nn.Module):\n",
    "    \n",
    "    ### in_dim: dimension of input; h_dim: number of neurons; out_dim: dimension of output\n",
    "    \n",
    "    def __init__(self, in_dim , h_dim , out_dim):\n",
    "        super(NeuralNet_Shallow, self).__init__()\n",
    "        self.ln1 = nn.Linear( in_dim , h_dim )\n",
    "        self.act1 =nn.Sigmoid()\n",
    "        #self.act1 =nn.Tanh()\n",
    "        #self.act1 =nn.ReLU()\n",
    "        \n",
    "        self.ln2 = nn.Linear( h_dim , out_dim , bias=False )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.ln1(x)\n",
    "        out = self.act1(out)\n",
    "        out = self.ln2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86133701",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet_Deep(torch.nn.Module):\n",
    "    \n",
    "    ### in_dim: dimension of input; h_dim: number of neurons; out_dim: dimension of output\n",
    "    ### depth: depth of the network\n",
    "    def __init__(self, in_dim , h_dim , out_dim , depth ):\n",
    "        super(NeuralNet_Deep, self).__init__()\n",
    "        self.depth = depth - 1\n",
    "        self.list  = nn.ModuleList()\n",
    "        self.ln1   = nn.Linear( in_dim , h_dim )\n",
    "        self.act1 =nn.Sigmoid()\n",
    "        #self.act1  = nn.Tanh()\n",
    "        \n",
    "        for i in range(self.depth):\n",
    "            self.list.append( nn.Linear( h_dim , h_dim ) )\n",
    "        \n",
    "        self.lnd = nn.Linear( h_dim , out_dim , bias=False )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.ln1(x)\n",
    "        out = self.act1(out)\n",
    "        for i in range(self.depth):\n",
    "            out = self.list[i](out)\n",
    "            out = self.act1(out)\n",
    "        out = self.lnd(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44af594",
   "metadata": {},
   "source": [
    "### Essential namedtuples in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79c82579",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataInput = namedtuple( \"DataInput\" , [ \"Xd\" , \"Phid\" , \"Btad\" , \"Fd\" , \"Xb\" , \"Ub\" , \"Xg\" , \"Fdng\" , \"NL\" , \"NL_sqrt\"] )\n",
    "LM_Setup  = namedtuple( \"LM_Setup\" , [ 'p_vec_o' , 'dp_o' , 'L_o' , 'J_o' , 'mu0' , 'criterion' ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6a18ed",
   "metadata": {},
   "source": [
    "### Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3929e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(iopt, Nd, Nb, Ng):\n",
    "    ## Xd: points inside the domain (randomly)\n",
    "    sampling = lhs(n_input, Nd)\n",
    "    xd = bda + (bdb-bda)*sampling[:,0:1]\n",
    "    yd = bdc + (bdd-bdc)*sampling[:,1:2]\n",
    "    zd = bde + (bdf-bde)*sampling[:,2:3]\n",
    "    Phid, qo = levfun(1, xd, yd, zd)\n",
    "    # remove points at the interface\n",
    "    qid  = np.where(np.abs(qo)<5e-15)\n",
    "    Phid = np.delete(Phid, qid[0], 0)\n",
    "    xd   = np.delete(xd, qid[0], 0)\n",
    "    yd   = np.delete(yd, qid[0], 0)\n",
    "    zd   = np.delete(zd, qid[0], 0)\n",
    "    qo   = np.delete(qo, qid[0], 0) \n",
    "    # beta\n",
    "    Btad = torch.tensor( btafun(xd, yd, zd, qo) ).double().to(device)\n",
    "    # torch tensor and requires_grad\n",
    "    Xd = np.hstack( ( xd , yd , zd , Phid[:,0:1] ) )\n",
    "    Xd = torch.tensor(Xd, requires_grad=True).double().to(device)\n",
    "    Phid = torch.tensor(Phid).double().to(device)\n",
    "    # right hand side\n",
    "    Fd = torch.tensor( lapu(xd, yd, zd, qo) ).double().to(device)\n",
    "\n",
    "    ## Xb: boundary points\n",
    "    xb = np.linspace(bda+dxh, bdb-dxh, Nb)\n",
    "    yb = np.linspace(bdc+dyh, bdd-dyh, Nb)\n",
    "    zb = np.linspace(bde+dzh, bdf-dzh, Nb)\n",
    "    # Uniformly generated mesh on the surface of box\n",
    "    [zz, yy] = np.meshgrid(zb, yb)\n",
    "    yy = yy.flatten()[:,None]\n",
    "    zz = zz.flatten()[:,None]\n",
    "    xx = np.ones_like(yy)\n",
    "    x0yz = np.hstack( ( bda*xx , yy , zz ) )\n",
    "    x1yz = np.hstack( ( bdb*xx , yy , zz ) )\n",
    "    [xx, zz] = np.meshgrid(xb, zb)\n",
    "    xx = xx.flatten()[:,None]\n",
    "    zz = zz.flatten()[:,None]\n",
    "    yy = np.ones_like(zz)\n",
    "    y0zx = np.hstack( ( xx , bdc*yy , zz ) )\n",
    "    y1zx = np.hstack( ( xx , bdd*yy , zz ) ) \n",
    "    [yy, xx] = np.meshgrid(yb, xb)\n",
    "    xx = xx.flatten()[:,None]\n",
    "    yy = yy.flatten()[:,None]\n",
    "    zz = np.ones_like(xx)\n",
    "    z0xy = np.hstack( ( xx , yy , bde*zz ) )\n",
    "    z1xy = np.hstack( ( xx , yy , bdf*zz ) )\n",
    "    # combine all faces\n",
    "    Xb = np.vstack( [ x0yz , x1yz , y0zx , y1zx , z0xy , z1xy ] )\n",
    "    q, qo = levfun(0, Xb[:,0:1], Xb[:,1:2], Xb[:,2:3])\n",
    "    # Boundary condition\n",
    "    Ub = exact_u( Xb[:,0:1] , Xb[:,1:2] , Xb[:,2:3] , qo)\n",
    "    Xb = np.hstack( ( Xb , q ) )\n",
    "    # torch tensor and require grad\n",
    "    Xb = torch.tensor(Xb, requires_grad=True).double().to(device)\n",
    "    Ub = torch.tensor(Ub).double().to(device)\n",
    "    \n",
    "    ## X_gma: points on the interface N_trg\n",
    "    X_tsg = np.loadtxt(fname_gma_tr, dtype='float64')\n",
    "    x = X_tsg[:,0:1]\n",
    "    y = X_tsg[:,1:2]\n",
    "    z = X_tsg[:,2:3]\n",
    "    q = np.zeros_like(x)\n",
    "    ## interface \n",
    "    X_tsg = np.hstack( ( x , y , z , q ) )\n",
    "    xgid = np.random.choice(range(np.size(X_tsg,0)),Ng)[:,None]\n",
    "    Xg   = X_tsg[xgid].squeeze(1)\n",
    "    # normal vector along the interface\n",
    "    Xnor_gm = lvnorvec(Xg[:,0:1], Xg[:,1:2], Xg[:,2:3])\n",
    "    # normal derivative jump condition for level function\n",
    "    Qdn_gm  = jump_lvdn(Xg[:,0:1], Xg[:,1:2], Xg[:,2:3])\n",
    "    # normal derivative jump condition for u\n",
    "    Udn_gm  = jump_btadun(Xg[:,0:1], Xg[:,1:2], Xg[:,2:3])     \n",
    "    # combine and torch tensor\n",
    "    Xg = torch.tensor(Xg, requires_grad=True).double().to(device)\n",
    "    Fdng = torch.tensor( np.hstack( ( Xnor_gm , Qdn_gm , Udn_gm ) ) ).double().to(device)\n",
    "    print(Fdng.shape)\n",
    "    print(Xg.shape)\n",
    "    \n",
    "    NL      = [len(Fd)+len(Ub)+len(Udn_gm), len(Fd), len(Ub), len(Udn_gm)]\n",
    "    NL_sqrt = np.sqrt(NL)\n",
    "\n",
    "    ## plot the distribution of data\n",
    "    if iopt != 0:\n",
    "        print(f'No. of training points in the bulk domain: {len(Fd)}')\n",
    "        print(f'No. of training points at the outer boundary: {len(Ub)}')\n",
    "        print(f'No. of training points at the interface: {Ng}')\n",
    "    \n",
    "    return Xd, Phid, Btad, Fd, Xb, Ub, Xg, Fdng, NL, NL_sqrt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc56ba3",
   "metadata": {},
   "source": [
    "### Components of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f28ad182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the interior loss\n",
    "def func_lossd(func_params, pts, phid, btad, fd):\n",
    "    def f(x, func_params):\n",
    "        fx = func_model(func_params, x)\n",
    "        return fx.squeeze(0).squeeze(0)\n",
    "    # derivatives of u wrt inputs\n",
    "    \n",
    "    d1u = jacrev(f)(pts, func_params)\n",
    "    d2u = jacrev(jacrev(f))(pts, func_params)\n",
    "    \n",
    "    u_x  = d1u[0]\n",
    "    u_y  = d1u[1]\n",
    "    u_z  = d1u[2]\n",
    "    u_q  = d1u[3]\n",
    "    \n",
    "    u_xq = d2u[0][:][3]\n",
    "    u_xx = d2u[0][:][0]\n",
    "    u_yq = d2u[1][:][3] \n",
    "    u_yy = d2u[1][:][1]\n",
    "    u_zq = d2u[2][:][3]\n",
    "    u_zz = d2u[2][:][2]\n",
    "    u_qq = d2u[3][:][3]\n",
    "    \n",
    "    lossd = btad[0:1]*( u_xx + u_yy + u_zz + 2.*( u_xq*phid[1:2] + u_yq*phid[2:3] + u_zq*phid[3:4] ) \\\n",
    "                       + u_qq*phid[4:5] + u_q*phid[5:6] ) \\\n",
    "            + btad[1:2]*(u_x+u_q*phid[1:2]) + btad[2:3]*(u_y+u_q*phid[2:3]) + btad[3:4]*(u_z+u_q*phid[3:4]) - fd\n",
    "    return lossd\n",
    "\n",
    "\n",
    "# compute the boundary loss \n",
    "def func_lossb(func_params, pts, ub):\n",
    "    def f(x, func_params):\n",
    "        fx = func_model(func_params, x)\n",
    "        return fx.squeeze(0).squeeze(0)\n",
    "    # function value at the boundary (Dirichlet boundary condition)\n",
    "    lossb = f(pts, func_params) - ub\n",
    "    return lossb\n",
    "\n",
    "\n",
    "# compute the interior loss\n",
    "def func_lossg(func_params, pts, fdng):\n",
    "    def f(x, func_params):\n",
    "        fx = func_model(func_params, x)\n",
    "        return fx.squeeze(0).squeeze(0)\n",
    "    # derivatives of u wrt inputs\n",
    "    \n",
    "    d1u = jacrev(f)(pts, func_params)\n",
    "    \n",
    "    ug_x = d1u[0]\n",
    "    ug_y = d1u[1]\n",
    "    ug_z = d1u[2]\n",
    "    ug_q = d1u[3]\n",
    "    \n",
    "    lossg = bta_jmp*(ug_x*fdng[0:1]+ug_y*fdng[1:2]+ug_z*fdng[2:3]) + bta_avg*ug_q*fdng[3:4] - fdng[4:5]\n",
    "    return lossg        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb3469",
   "metadata": {},
   "source": [
    "### Levenberg-Marquardt (LM) optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eb884d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters counter\n",
    "def count_parameters(func_params):\n",
    "    return sum(p.numel() for p in func_params if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dddc3aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model's parameter\n",
    "def get_p_vec(func_params):\n",
    "    p_vec = []\n",
    "    cnt = 0\n",
    "    for p in func_params:\n",
    "        p_vec = p.contiguous().view(-1) if cnt == 0 else torch.cat([p_vec, p.contiguous().view(-1)])\n",
    "        cnt = 1 \n",
    "    return p_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d54fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of LM method\n",
    "def generate_initial_LM(func_params, Xd_len, Xb_len, Xg_len):\n",
    "    \n",
    "    # data_length\n",
    "    data_length = Xd_len + Xb_len + Xg_len\n",
    "    \n",
    "    # p_vector\n",
    "    with torch.no_grad():\n",
    "        p_vec_old = get_p_vec(func_params).double().to(device)\n",
    "    \n",
    "    # dp\n",
    "    dp_old = torch.zeros( [ count_parameters(func_params) , 1 ] ).double().to(device)\n",
    "\n",
    "    # Loss\n",
    "    L_old = torch.zeros( [ data_length , 1 ] ).double().to(device)\n",
    "    \n",
    "    # Jacobian\n",
    "    J_old = torch.zeros( [ data_length , count_parameters(func_params) ] ).double().to(device)\n",
    "    \n",
    "    return p_vec_old, dp_old, L_old, J_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "641f3a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PINNs_LM(func_params, LM_setup, tr_input, lossval, lossval_dbg):\n",
    "    \n",
    "    # assign tuple elements of LM_set_up\n",
    "    p_vec_o, dp_o, L_o, J_o, mu, criterion = LM_setup\n",
    "    I_pvec = torch.eye(len(p_vec_o)).to(device)\n",
    "    \n",
    "    # assign tuple elements of data_input \n",
    "    [Xd, Phid, Btad, Fd, Xb, Ub, Xg, Fdng, NL, NL_sqrt] = tr_input\n",
    "    \n",
    "    # iteration counts and check\n",
    "    Comput_old = True\n",
    "    step       = 0\n",
    "    \n",
    "    # try-except statement to avoid jam in the code\n",
    "    try:\n",
    "        while (lossval[-1]>tol_main) and (step<=tr_iter_max):\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            ############################################################\n",
    "            # LM_optimizer\n",
    "            if ( Comput_old == True ):   # need to compute loss_old and J_old\n",
    "                \n",
    "                ### computation of loss\n",
    "                Ld = vmap((func_lossd), (None, 0, 0, 0, 0))(func_params, Xd, Phid, Btad, Fd).flatten().detach()\n",
    "                Lb = vmap((func_lossb), (None, 0, 0))(func_params, Xb, Ub).flatten().detach()\n",
    "                Lg = vmap((func_lossg), (None, 0, 0))(func_params, Xg, Fdng).flatten().detach()\n",
    "                L  = torch.cat( ( Ld/NL_sqrt[1] , Lb/NL_sqrt[2] , Lg/NL_sqrt[3] ) )\n",
    "                L  = L.reshape(NL[0],1).detach()\n",
    "                lsd_sum = torch.sum(Ld*Ld)/NL[1]\n",
    "                lsb_sum = torch.sum(Lb*Lb)/NL[2]\n",
    "                lsg_sum = torch.sum(Lg*Lg)/NL[3]\n",
    "                loss_dbg_old = [lsd_sum.item(), lsb_sum.item(), lsg_sum.item()]\n",
    "                        \n",
    "            loss_old     = lossval[-1]\n",
    "            loss_dbg_old = lossval_dbg[-1]\n",
    "    \n",
    "            ### compute the gradinet of loss function for each point\n",
    "            with torch.no_grad():\n",
    "                p_vec = get_p_vec(func_params).detach() # get p_vec for p_vec_old if neccessary \n",
    "        \n",
    "            if criterion:\n",
    "                per_sample_grads_D = vmap(jacrev(func_lossd), (None, 0, 0, 0, 0))(func_params, Xd, Phid, Btad, Fd)\n",
    "                cnt = 0\n",
    "                for g in per_sample_grads_D: \n",
    "                    g = g.detach()\n",
    "                    J_d = g.reshape(len(g),-1) if cnt == 0 else torch.hstack([J_d,g.reshape(len(g),-1)])\n",
    "                    cnt = 1\n",
    "\n",
    "                per_sample_grads_B = vmap(jacrev(func_lossb), (None, 0, 0))(func_params, Xb, Ub)\n",
    "                cnt = 0\n",
    "                for g in per_sample_grads_B: \n",
    "                    g = g.detach()\n",
    "                    J_b = g.reshape(len(g),-1) if cnt == 0 else torch.hstack([J_b,g.reshape(len(g),-1)])\n",
    "                    cnt = 1\n",
    "        \n",
    "                per_sample_grads_G = vmap(jacrev(func_lossg), (None, 0, 0))(func_params, Xg, Fdng)\n",
    "                cnt = 0\n",
    "                for g in per_sample_grads_G: \n",
    "                    g = g.detach()\n",
    "                    J_g = g.reshape(len(g),-1) if cnt == 0 else torch.hstack([J_g,g.reshape(len(g),-1)])\n",
    "                    cnt = 1\n",
    "            \n",
    "                J = torch.cat( ( J_d/NL_sqrt[1] , J_b/NL_sqrt[2] , J_g/NL_sqrt[3] ) ).detach()\n",
    "                \n",
    "                ### info. normal equation of J\n",
    "                J_product = J.t()@J\n",
    "                rhs       = - J.t()@L            \n",
    "                 \n",
    "                    \n",
    "            with torch.no_grad():\n",
    "                ### solve the linear system\n",
    "                dp  = torch.linalg.solve( J_product + mu*I_pvec , rhs )\n",
    "                cnt = 0\n",
    "                for p in func_params:\n",
    "                    mm   = torch.Tensor([p.shape]).tolist()[0]\n",
    "                    num  = int( functools.reduce( lambda x,y : x*y, mm, 1 ) )\n",
    "                    p   += dp[cnt:cnt+num].reshape(p.shape)\n",
    "                    cnt += num\n",
    "            \n",
    "            ### Compute loss_new    \n",
    "            Ld = vmap((func_lossd), (None, 0, 0, 0, 0))(func_params, Xd, Phid, Btad, Fd).flatten().detach()\n",
    "            Lb = vmap((func_lossb), (None, 0, 0))(func_params, Xb, Ub).flatten().detach()\n",
    "            Lg = vmap((func_lossg), (None, 0, 0))(func_params, Xg, Fdng).flatten().detach()\n",
    "            L  = torch.cat( ( Ld/NL_sqrt[1] , Lb/NL_sqrt[2] , Lg/NL_sqrt[3] ) )\n",
    "            L  = L.reshape( NL[0] , 1 ).detach()\n",
    "            loss_new = torch.sum(L*L).item()\n",
    "            lsd_sum = torch.sum(Ld*Ld)/NL[1]\n",
    "            lsb_sum = torch.sum(Lb*Lb)/NL[2]\n",
    "            lsg_sum = torch.sum(Lg*Lg)/NL[3]\n",
    "            loss_dbg_new = [lsd_sum.item(), lsb_sum.item(), lsg_sum.item()]\n",
    "    \n",
    "                \n",
    "            # strategy to update mu\n",
    "            if ( step > 0 ):\n",
    "                \n",
    "                with torch.no_grad():\n",
    "             \n",
    "                    # accept update \n",
    "                    if loss_new < loss_old:\n",
    "                        p_vec_old  = p_vec.detach()\n",
    "                        dp_old     = dp\n",
    "                        L_old      = L\n",
    "                        J_old      = J\n",
    "                        mu         = max( mu/1.5 , tol_machine )\n",
    "                        criterion  = True #False\n",
    "                        Comput_old = False\n",
    "                        lossval.append(loss_new)\n",
    "                        lossval_dbg.append(loss_dbg_new)\n",
    "                    \n",
    "                    else:\n",
    "                        cosine = nn.functional.cosine_similarity(dp, dp_old, dim=0, eps=1e-15)\n",
    "                        cosine_check = (1.-cosine)*loss_new > min(lossval) # loss_old\n",
    "                        if cosine_check: # give up the direction\n",
    "                            cnt=0\n",
    "                            for p in func_params:\n",
    "                                mm   = torch.Tensor([p.shape]).tolist()[0]\n",
    "                                num  = int( functools.reduce(lambda x,y: x*y, mm, 1) )\n",
    "                                p   -= dp[cnt:cnt+num].reshape(p.shape)\n",
    "                                cnt += num\n",
    "                            mu = min( 2*mu , mu_max )\n",
    "                            criterion  = False\n",
    "                            Comput_old = False\n",
    "                        else: # accept \n",
    "                            p_vec_old = p_vec.detach()\n",
    "                            dp_old    = dp \n",
    "                            L_old     = L\n",
    "                            J_old     = J\n",
    "                            mu        = max( mu/1.5 , tol_machine )       \n",
    "                            criterion  = True\n",
    "                            Comput_old = False\n",
    "                        lossval.append(loss_old)\n",
    "                        lossval_dbg.append(loss_dbg_old)\n",
    "            \n",
    "            else:   # for old info. \n",
    "       \n",
    "                with torch.no_grad():\n",
    "              \n",
    "                    p_vec_old  = p_vec.detach()\n",
    "                    dp_old     = dp\n",
    "                    L_old      = L\n",
    "                    J_old      = J\n",
    "                    mu         = max( mu/1.5 , tol_machine )\n",
    "                    criterion  = True\n",
    "                    Comput_old = False\n",
    "                    lossval.append(loss_new)\n",
    "                    lossval_dbg.append(loss_dbg_new)\n",
    "            \n",
    "\n",
    "            if step % ls_check == ls_check0:\n",
    "                print(\"Step %s: \" % (step) )\n",
    "                print(f\" training loss: {lossval[-1]:.4e}\")\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "        \n",
    "        print(\"Step %s: \" % (step-1) )\n",
    "        print(f\" training loss: {lossval[-1]:.4e}\")\n",
    "        print('finished')\n",
    "        lossval     = lossval[1:]\n",
    "        lossval_dbg = lossval_dbg[1:]\n",
    "        relerr_loss = lossval[-1]\n",
    "        return lossval, lossval_dbg, relerr_loss\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print('Interrupt')\n",
    "        print('steps = ', step)\n",
    "        lossval     = lossval[1:]\n",
    "        lossval_dbg = lossval_dbg[1:]\n",
    "        relerr_loss = lossval[-1]\n",
    "        return lossval, lossval_dbg, relerr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "433c5d45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160, 5])\n",
      "torch.Size([160, 4])\n",
      "No. of training points in the bulk domain: 800\n",
      "No. of training points at the outer boundary: 2400\n",
      "No. of training points at the interface: 160\n",
      "No. of trainable parameters = 234\n",
      "Step 499: \n",
      " training loss: 2.7408e-07\n",
      "Step 999: \n",
      " training loss: 1.3506e-07\n",
      "Step 1499: \n",
      " training loss: 1.0079e-08\n",
      "Step 1999: \n",
      " training loss: 4.1865e-09\n",
      "Step 2499: \n",
      " training loss: 3.5277e-09\n",
      "Step 2999: \n",
      " training loss: 3.1375e-09\n",
      "Step 3000: \n",
      " training loss: 3.1375e-09\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "relerr_loss = []\n",
    "for char in char_id:\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "    if n_depth==1:   # Shallow NN\n",
    "        model = NeuralNet_Shallow( n_input , n_hidden , n_output ).double().to(device)\n",
    "    else:   # Deep NN\n",
    "        model = NeuralNet_Deep( n_input , n_hidden , n_output , n_depth ).double().to(device)\n",
    "\n",
    "    # use Pytorch and functorch\n",
    "    func_model, func_params = make_functional(model)\n",
    "    \n",
    "    # generate training data\n",
    "    Xd_tr, Phid_tr, Btad_tr, Fd_tr, Xb_tr, Ub_tr, Xg_tr, Fdng_tr, NL_tr, NL_sqrt_tr = generate_data(1, N_trd, N_trb, N_trg)\n",
    "    tr_input = DataInput(Xd=Xd_tr, Phid=Phid_tr, Btad=Btad_tr, Fd=Fd_tr, Xb=Xb_tr, Ub=Ub_tr, \n",
    "                         Xg=Xg_tr, Fdng=Fdng_tr, NL=NL_tr, NL_sqrt=NL_sqrt_tr)\n",
    "    \n",
    "    # initialization of LM\n",
    "    p_vec_old, dp_old, L_old, J_old = generate_initial_LM(func_params, NL_tr[1], NL_tr[2], NL_tr[3])\n",
    "    print(f\"No. of trainable parameters = {len(p_vec_old)}\")\n",
    "        \n",
    "    # LM_setup\n",
    "    mu = 10**(8)\n",
    "    criterion = True\n",
    "    LM_setup = LM_Setup( p_vec_o=p_vec_old , dp_o=dp_old , L_o=L_old , J_o=J_old , mu0=mu , criterion=criterion )\n",
    "\n",
    "    # allocate loss\n",
    "    lossval         = []\n",
    "    lossval_dbg     = []\n",
    "    lossval.append(1.)\n",
    "    lossval_dbg.append([1.,1.,1.])\n",
    "\n",
    "    # train the model by LM optimizer\n",
    "    lossval, lossval_dbg, relerr_loss_char = train_PINNs_LM(func_params, LM_setup, tr_input, lossval, lossval_dbg)\n",
    "    relerr_loss.append(relerr_loss_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf8a740b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_loss   = len(lossval)\n",
    "lossval  = np.array(lossval).reshape(N_loss,1)\n",
    "epochcol = np.linspace(1, N_loss, N_loss).reshape(N_loss,1)\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "\n",
    "plt.semilogy(epochcol, lossval)\n",
    "plt.title('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe66702",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dfba869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of testing point: N_tsin (800000) + N_tsgm (23230) = 823230\n"
     ]
    }
   ],
   "source": [
    "# fixed pseudo random generator\n",
    "np.random.seed(777)\n",
    "\n",
    "# testing points\n",
    "sampling = lhs(n_input-1, N_tsd_final)\n",
    "x = bda + (bdb-bda)*sampling[:,0:1]\n",
    "y = bdc + (bdd-bdc)*sampling[:,1:2]\n",
    "z = bde + (bdf-bde)*sampling[:,2:3]\n",
    "q, qo  = levfun(0, x, y, z)\n",
    "X_tsd = np.hstack( ( x , y , z, q ) )\n",
    "X_tsd = torch.tensor(X_tsd).double().to(device)\n",
    "\n",
    "# exact solution of u\n",
    "ref_u_tsd = exact_u(x, y, z, qo)\n",
    "ref_u_tsd_infnorm = np.linalg.norm(ref_u_tsd, np.inf)\n",
    "ref_u_tsd_L2norm  = np.linalg.norm(ref_u_tsd, 2)\n",
    "\n",
    "# generate interfacial testing points\n",
    "X_tsg = np.loadtxt(fname_gma_tr, dtype='float64')\n",
    "x = X_tsg[:,0:1]\n",
    "y = X_tsg[:,1:2]\n",
    "z = X_tsg[:,2:3]\n",
    "q = np.zeros_like(x)\n",
    "## normal vector \n",
    "Xnor_tsg = lvnorvec(x, y, z)\n",
    "## normal derivative jump condition for level function\n",
    "qdn_tsg = jump_lvdn(x, y, z)\n",
    "## normal derivative jump condition for u\n",
    "udn_tsg = jump_btadun(x, y, z)\n",
    "## interface \n",
    "X_tsg = np.hstack( ( x , y , z , q ) )\n",
    "\n",
    "# No. of testing points\n",
    "N_tsg_final = len(q)\n",
    "print(f'No. of testing point: N_tsin ({N_tsd_final}) + N_tsgm ({N_tsg_final}) = {N_tsd_final+N_tsg_final}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19f71d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_inf err.    : 6.4299e-06\n",
      "L_inf relerr. : 2.5011e-05\n",
      "L_2 err.    : 1.5929e-03\n",
      "L_2 relerr. : 7.2205e-06\n"
     ]
    }
   ],
   "source": [
    "pred_u = func_model(func_params, X_tsd)[:].cpu().detach().numpy().flatten()\n",
    "abserr = np.abs( pred_u - ref_u_tsd.flatten() )\n",
    "err_inf = np.linalg.norm(abserr, np.inf)\n",
    "err_L2  = np.linalg.norm(abserr, 2)\n",
    "relerr_inf = err_inf / ref_u_tsd_infnorm\n",
    "print(f\"L_inf err.    : {err_inf:.4e}\")\n",
    "print(f\"L_inf relerr. : {relerr_inf:.4e}\")\n",
    "relerr_L2  = err_L2 / ref_u_tsd_L2norm\n",
    "print(f\"L_2 err.    : {err_L2:.4e}\")\n",
    "print(f\"L_2 relerr. : {relerr_L2:.4e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
