{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243f5b05",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cusp-capturing PINN - Example 5\n",
    "\n",
    "* level set augmentation: $\\phi_a = |\\phi|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d1b38f",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b603d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "from pyDOE import lhs\n",
    "from scipy.stats.distributions import norm as spnorm\n",
    "\n",
    "from functorch import make_functional, vmap, grad, jacrev, hessian\n",
    "\n",
    "from collections import namedtuple, OrderedDict\n",
    "import datetime\n",
    "import time\n",
    "from IPython.display import clear_output \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d37a549",
   "metadata": {},
   "source": [
    "### Empty cache and check devices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f40cfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cpu\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device = ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85db90d",
   "metadata": {},
   "source": [
    "### Pre-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96c944de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit sphere domain S^d\n",
    "dims  = 6\n",
    "dims0 = dims - 1\n",
    "Rad   = .6\n",
    "# radii of the sphere\n",
    "rk = .5\n",
    "rsq = rk * rk\n",
    "# input_size\n",
    "input_size = dims + 1\n",
    "# \\beta^+, \\beta^-, and \\eta\n",
    "btao = 1.\n",
    "btai = 1.\n",
    "bta_jmp = btao - btai\n",
    "bta_avg = 0.5*( btao + btai )\n",
    "# Network size\n",
    "n_input0 = dims\n",
    "n_input  = n_input0 + 1\n",
    "n_hidden = 40\n",
    "n_output = 1\n",
    "n_depth  = 1 # only used in deep NN\n",
    "# tolerence for LM\n",
    "tol_main    = 10**(-10)\n",
    "tol_machine = 10**(-15)\n",
    "mu_max      = 10**8\n",
    "mu_ini      = 10**8\n",
    "mu_div      = 1.3\n",
    "mu_mul      = 2.\n",
    "# iteration counts and check\n",
    "tr_iter_max    = 3000                      # max. iteration\n",
    "ts_input_new   = 500                       # renew testing points \n",
    "ls_check       = 500\n",
    "ls_check0      = ls_check - 1\n",
    "# number of training points and testing points\n",
    "c_addpt = 1.\n",
    "N_trd = 500\n",
    "N_trb = int( 6*(N_trd)**(5./6.) )\n",
    "N_trg = N_trb\n",
    "N_tsd_final = 100*N_trd\n",
    "N_tsb_final = 100*N_trb\n",
    "N_tsg_final = 100*N_trg\n",
    "# create names for storages\n",
    "fname = 'test'\n",
    "char_id = 'a'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7c8f23",
   "metadata": {},
   "source": [
    "### Exact solution, level set function, and right-hand-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90444c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levfun(opt, x):\n",
    "    nr = np.size(x,0)\n",
    "    qo = np.zeros((nr,1))\n",
    "    qo[:,0] = np.sum(x*x, axis=1)/rsq - 1.0\n",
    "    if opt == 0:\n",
    "        q = np.zeros_like(qo)\n",
    "        for i in range(len(qo)):\n",
    "            if qo[i] >= 0.:\n",
    "                q[i] = qo[i]\n",
    "            else:\n",
    "                q[i] = - qo[i]\n",
    "    else:\n",
    "        nc = np.size(x,1);\n",
    "        q  = np.zeros( (nr,nc+3) )\n",
    "        q[:,0:1]  = qo\n",
    "        q[:,1:-2] = 2*x/rsq\n",
    "        q[:,-1]   = 2*nc/rsq\n",
    "        for i in range(nr):\n",
    "            if qo[i] >= 0.:\n",
    "                q[i,:] = q[i,:]\n",
    "            else:\n",
    "                q[i,:] = - q[i,:]\n",
    "        q[:,-2] = np.sum( q[:,1:-2]*q[:,1:-2] , axis=1 )\n",
    "    return qo, q\n",
    "\n",
    "\n",
    "def lvnorvec(x):\n",
    "    qnor   = x/rsq\n",
    "    dqsqrt = np.sqrt( np.sum(qnor*qnor,axis=1) ).reshape((len(x[:,0]),1))\n",
    "    qnor   = qnor/dqsqrt  \n",
    "    return qnor\n",
    "\n",
    "\n",
    "# exact solution of example 01\n",
    "def exact_u(x, z):\n",
    "    eu = np.sum( np.sin(x[:,0:-1]) , axis=1 ).reshape( (len(z),1) )\n",
    "    dd = rsq - np.sum( x*x , axis=1 )\n",
    "    for i in range(len(z)):\n",
    "        if z[i] >= 0.:\n",
    "            eu[i] += np.exp( dd[i] )\n",
    "        else:\n",
    "            eu[i] += 1. + 2*np.sin( dd[i] )\n",
    "    return eu\n",
    "\n",
    "\n",
    "def exact_b(x, z):\n",
    "    eb = np.zeros_like(z)\n",
    "    for i in range(len(z)):\n",
    "        if z[i] >= 0.:\n",
    "            eb[i] = btao\n",
    "        else:\n",
    "            eb[i] = btai\n",
    "    return eb\n",
    "\n",
    "\n",
    "# normal derivative jump of level set function\n",
    "def jump_lvdn(x):\n",
    "    qdn_jmp = (4/rsq)* np.sqrt( np.sum( x*x , axis=1 ) ).reshape((len(x[:,0]),1))\n",
    "    return qdn_jmp\n",
    "\n",
    "\n",
    "# normal derivative jump condition along interface: [\\beta\\partial_n u]=0 \n",
    "def jump_btadun(x):\n",
    "    dun = rk*(4*btai-2*btao) + (bta_jmp/rk)*np.sum( np.cos(x[:,0:-1])*x[:,0:-1]  , axis=1 )\n",
    "    dun = dun.reshape((len(x[:,0]),1))\n",
    "    return dun\n",
    "\n",
    "\n",
    "# source\n",
    "def lapu(x, z):\n",
    "    ef = - np.sum( np.sin(x[:,0:-1]) , axis=1 ).reshape( (len(z),1) )\n",
    "    dd = np.sum( x*x , axis=1 ).reshape( (len(z),1) )\n",
    "    ep = rsq - dd\n",
    "    cs = np.cos( ep )\n",
    "    ss = np.sin( ep )\n",
    "    ep = np.exp( ep )\n",
    "    for i in range(len(z)):\n",
    "        if z[i] >= 0.:\n",
    "            ef[i] += ( 4*dd[i] - 12. )*ep[i]\n",
    "        else:\n",
    "            ef[i] += - 8*dd[i]*ss[i] - 24*cs[i]            \n",
    "    return ef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c37cfb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define networks: Shallow and Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44879550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet_Shallow(torch.nn.Module):\n",
    "    \n",
    "    ### in_dim: dimension of input; h_dim: number of neurons; out_dim: dimension of output\n",
    "    \n",
    "    def __init__(self, in_dim , h_dim , out_dim):\n",
    "        super(NeuralNet_Shallow, self).__init__()\n",
    "        self.ln1 = nn.Linear( in_dim , h_dim )\n",
    "        self.act1 =nn.Sigmoid()\n",
    "        #self.act1 =nn.Tanh()\n",
    "        #self.act1 =nn.ReLU()\n",
    "        \n",
    "        self.ln2 = nn.Linear( h_dim , out_dim , bias=False )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.ln1(x)\n",
    "        out = self.act1(out)\n",
    "        out = self.ln2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a23e3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet_Deep(torch.nn.Module):\n",
    "    \n",
    "    ### in_dim: dimension of input; h_dim: number of neurons; out_dim: dimension of output\n",
    "    ### depth: depth of the network\n",
    "    def __init__(self, in_dim , h_dim , out_dim , depth ):\n",
    "        super(NeuralNet_Deep, self).__init__()\n",
    "        self.depth = depth - 1\n",
    "        self.list  = nn.ModuleList()\n",
    "        self.ln1   = nn.Linear( in_dim , h_dim )\n",
    "        self.act1 =nn.Sigmoid()\n",
    "        #self.act1  = nn.Tanh()\n",
    "        \n",
    "        for i in range(self.depth):\n",
    "            self.list.append( nn.Linear( h_dim , h_dim ) )\n",
    "        \n",
    "        self.lnd = nn.Linear( h_dim , out_dim , bias=False )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.ln1(x)\n",
    "        out = self.act1(out)\n",
    "        for i in range(self.depth):\n",
    "            out = self.list[i](out)\n",
    "            out = self.act1(out)\n",
    "        out = self.lnd(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44af594",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Essential namedtuples in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6dbe6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataInput = namedtuple( \"DataInput\" , [ \"Xd\" , \"Fd\" , \"Xb\" , \"Fb\" , \"Xg\" , \"Fg\" , \"NL\" , \"NL_sqrt\"] )\n",
    "LM_Setup  = namedtuple( \"LM_Setup\" , [ 'p_vec_o' , 'dp_o' , 'L_o' , 'J_o' , 'mu0' , 'criterion' ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6a18ed",
   "metadata": {},
   "source": [
    "### Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8dad94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(iopt, Nd, Nb, Ng):\n",
    "    \n",
    "    # Xd: interior points \n",
    "    r_d = lhs(1, samples=Nd)\n",
    "    r_d = r_d**(1./dims)\n",
    "    Xd = spnorm( loc=0 , scale=1 ).ppf( lhs(dims,samples=Nd) )\n",
    "    Xd = Rad*Xd/np.expand_dims( np.linalg.norm(Xd,axis=1) , axis=1 )\n",
    "    Xd = ( r_d*Xd ).reshape( ( Nd , dims ) )\n",
    "    qo, q = levfun(1, Xd)\n",
    "    Fd = np.hstack( ( q , lapu(Xd, qo) ) )\n",
    "    Xd = np.hstack( ( Xd , q[:,0:1] ) )    \n",
    "    \n",
    "    # Xb: boundary points\n",
    "    Xb = lhs( dims , samples=Nb )\n",
    "    Xb = spnorm( loc=0 , scale=1 ).ppf(Xb)\n",
    "    Xb = Rad*Xb/np.expand_dims( np.linalg.norm(Xb,axis=1) , axis=1 )\n",
    "    Xb = Xb.reshape( ( Nb , dims ) )\n",
    "    qo, q = levfun(0, Xb)\n",
    "    Fb = exact_u(Xb, qo)\n",
    "    Xb = np.hstack( ( Xb , q ) )\n",
    "\n",
    "    # Xg: points on the interface\n",
    "    Xg = lhs(dims, samples=Ng)\n",
    "    Xg = spnorm( loc=0 , scale=1 ).ppf(Xg)\n",
    "    Xg = rk*Xg/np.expand_dims( np.linalg.norm(Xg,axis=1) , axis=1 )\n",
    "    Xg = Xg.reshape( ( Ng , dims ) )\n",
    "    qo = np.zeros( (len(Xg[:,0]),1) )\n",
    "    # normal vector along the interface\n",
    "    Xnor_gm = lvnorvec(Xg)\n",
    "    # normal derivative jump condition for level function\n",
    "    Qdn_gm  = jump_lvdn(Xg)\n",
    "    # normal derivative jump condition for u\n",
    "    Udn_gm  = jump_btadun(Xg)     \n",
    "    # combine and torch tensor\n",
    "    Fg = np.hstack( ( Xnor_gm , Qdn_gm , Udn_gm ) )\n",
    "    Xg = np.hstack( ( Xg , qo ) )\n",
    "   \n",
    "    # torch tensor and requires_grad\n",
    "    Xd = torch.tensor(Xd, requires_grad=True).double().to(device)\n",
    "    Xb = torch.tensor(Xb, requires_grad=True).double().to(device)\n",
    "    Xg = torch.tensor(Xg, requires_grad=True).double().to(device)\n",
    "    Fd = torch.tensor(Fd).double().to(device)\n",
    "    Fb = torch.tensor(Fb).double().to(device)\n",
    "    Fg = torch.tensor(Fg).double().to(device)\n",
    "    \n",
    "    NL      = [ Nd+Nb+Ng , Nd , Nb , Ng ]\n",
    "    NL_sqrt = np.sqrt(NL)\n",
    "    \n",
    "    if ( iopt == 1 ):\n",
    "        print(f'No. of training points in the bulk domain is {Nd}')\n",
    "        print(f'No. of training points at the outer boundary is {Nb}')\n",
    "        print(f'No. of training points at the interface is {Ng}')\n",
    "        print(f'No. of overall training points is {Nd+Nb+Ng}')\n",
    "        \n",
    "    \n",
    "    return Xd, Fd, Xb, Fb, Xg, Fg, NL, NL_sqrt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc56ba3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Components of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16ada3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the interior loss\n",
    "def func_lossd(func_params, pts, fd):\n",
    "    def f(x, func_params):\n",
    "        fx = func_model(func_params, x)\n",
    "        return fx.squeeze(0).squeeze(0)\n",
    "    # derivatives of u wrt inputs\n",
    "    \n",
    "    d1u = jacrev(f)(pts, func_params)\n",
    "    d2u = jacrev(jacrev(f))(pts, func_params)\n",
    "    \n",
    "    uq = d1u[6]\n",
    "    u11 = d2u[0][0]\n",
    "    u22 = d2u[1][1]\n",
    "    u33 = d2u[2][2]\n",
    "    u44 = d2u[3][3]\n",
    "    u55 = d2u[4][4]\n",
    "    u66 = d2u[5][5]\n",
    "    uqq = d2u[6][6]\n",
    "    u1q = d2u[0][6] \n",
    "    u2q = d2u[1][6]\n",
    "    u3q = d2u[2][6] \n",
    "    u4q = d2u[3][6]\n",
    "    u5q = d2u[4][6]\n",
    "    u6q = d2u[5][6]\n",
    "    lossd = u11 + u22 + u33 + u44 + u55 + u66 + 2.*( u1q*fd[1:2] + u2q*fd[2:3] + u3q*fd[3:4] + \\\n",
    "            u4q*fd[4:5] + u5q*fd[5:6] + u6q*fd[6:7] ) + uqq*fd[7:8] + uq*fd[8:9] - fd[9:10]\n",
    "    return lossd\n",
    "\n",
    "\n",
    "# compute the boundary loss \n",
    "def func_lossb(func_params, pts, fb):\n",
    "    def f(x, func_params):\n",
    "        fx = func_model(func_params, x)\n",
    "        return fx.squeeze(0).squeeze(0)\n",
    "    # function value at the boundary (Dirichlet boundary condition)\n",
    "    lossb = f(pts, func_params) - fb\n",
    "    return lossb\n",
    "\n",
    "\n",
    "# compute the interior loss\n",
    "def func_lossg(func_params, pts, fg):\n",
    "    def f(x, func_params):\n",
    "        fx = func_model(func_params, x)\n",
    "        return fx.squeeze(0).squeeze(0)\n",
    "    # derivatives of u wrt inputs\n",
    "    d1u = jacrev(f)(pts, func_params)\n",
    "    ug0 = d1u[0]\n",
    "    ug1 = d1u[1]\n",
    "    ug2 = d1u[2]\n",
    "    ug3 = d1u[3]\n",
    "    ug4 = d1u[4]\n",
    "    ug5 = d1u[5]\n",
    "    ugq = d1u[6]\n",
    "    lossg = bta_jmp*( ug0*fg[0:1] + ug1*fg[1:2] + ug2*fg[2:3] + ug3*fg[3:4] +  \\\n",
    "                      ug4*fg[4:5] + ug5*fg[5:6] ) + bta_avg*ugq*fg[6:7] - fg[7:8]\n",
    "    return lossg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb3469",
   "metadata": {},
   "source": [
    "### Levenberg-Marquardt (LM) optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99041c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters counter\n",
    "def count_parameters(func_params):\n",
    "    return sum(p.numel() for p in func_params if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88ba37be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model's parameter\n",
    "def get_p_vec(func_params):\n",
    "    p_vec = []\n",
    "    cnt = 0\n",
    "    for p in func_params:\n",
    "        p_vec = p.contiguous().view(-1) if cnt == 0 else torch.cat([p_vec, p.contiguous().view(-1)])\n",
    "        cnt = 1 \n",
    "    return p_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd0a7e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of LM method\n",
    "def generate_initial_LM(func_params, Xd_len, Xb_len, Xg_len):\n",
    "    \n",
    "    # data_length\n",
    "    data_length = Xd_len + Xb_len + Xg_len\n",
    "    \n",
    "    # p_vector\n",
    "    with torch.no_grad():\n",
    "        p_vec_old = get_p_vec(func_params).double().to(device)\n",
    "    \n",
    "    # dp\n",
    "    dp_old = torch.zeros( [ count_parameters(func_params) , 1 ] ).double().to(device)\n",
    "\n",
    "    # Loss\n",
    "    L_old = torch.zeros( [ data_length , 1 ] ).double().to(device)\n",
    "    \n",
    "    # Jacobian\n",
    "    J_old = torch.zeros( [ data_length , count_parameters(func_params) ] ).double().to(device)\n",
    "    \n",
    "    return p_vec_old, dp_old, L_old, J_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62431280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PINNs_LM(func_params, LM_setup, tr_input, lossval, lossval_dbg):\n",
    "    \n",
    "    # assign tuple elements of LM_set_up\n",
    "    p_vec_o, dp_o, L_o, J_o, mu, criterion = LM_setup\n",
    "    I_pvec = torch.eye(len(p_vec_o)).to(device)\n",
    "    \n",
    "    # assign tuple elements of data_input \n",
    "    [Xd, Fd, Xb, Fb, Xg, Fg, NL, NL_sqrt] = tr_input\n",
    "    \n",
    "    # iteration counts and check\n",
    "    Comput_old = True\n",
    "    step       = 0\n",
    "    \n",
    "    # try-except statement to avoid jam in the code\n",
    "    try:\n",
    "        while (lossval[-1]>tol_main) and (step<=tr_iter_max):\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            ############################################################\n",
    "            # LM_optimizer\n",
    "            if ( Comput_old == True ):   # need to compute loss_old and J_old\n",
    "                \n",
    "                ### computation of loss\n",
    "                Ld = vmap((func_lossd), (None, 0, 0))(func_params, Xd, Fd).flatten().detach()\n",
    "                Lb = vmap((func_lossb), (None, 0, 0))(func_params, Xb, Fb).flatten().detach()\n",
    "                Lg = vmap((func_lossg), (None, 0, 0))(func_params, Xg, Fg).flatten().detach()\n",
    "                L  = torch.cat( ( Ld/NL_sqrt[1] , Lb/NL_sqrt[2] , Lg/NL_sqrt[3] ) )\n",
    "                L  = L.reshape(NL[0],1).detach()\n",
    "                lsd_sum = torch.sum(Ld*Ld)/NL[1]\n",
    "                lsb_sum = torch.sum(Lb*Lb)/NL[2]\n",
    "                lsg_sum = torch.sum(Lg*Lg)/NL[3]\n",
    "                loss_dbg_old = [lsd_sum.item(), lsb_sum.item(), lsg_sum.item()]\n",
    "                        \n",
    "            loss_old     = lossval[-1]\n",
    "            loss_dbg_old = lossval_dbg[-1]\n",
    "    \n",
    "            ### compute the gradinet of loss function for each point\n",
    "            with torch.no_grad():\n",
    "                p_vec = get_p_vec(func_params).detach() # get p_vec for p_vec_old if neccessary \n",
    "        \n",
    "            if criterion:\n",
    "                per_sample_grads = vmap(jacrev(func_lossd), (None, 0, 0))(func_params, Xd, Fd)\n",
    "                cnt = 0\n",
    "                for g in per_sample_grads: \n",
    "                    g = g.detach()\n",
    "                    J_d = g.reshape(len(g),-1) if cnt == 0 else torch.hstack([J_d,g.reshape(len(g),-1)])\n",
    "                    cnt = 1\n",
    "\n",
    "                per_sample_grads = vmap(jacrev(func_lossb), (None, 0, 0))(func_params, Xb, Fb)\n",
    "                cnt = 0\n",
    "                for g in per_sample_grads: \n",
    "                    g = g.detach()\n",
    "                    J_b = g.reshape(len(g),-1) if cnt == 0 else torch.hstack([J_b,g.reshape(len(g),-1)])\n",
    "                    cnt = 1\n",
    "        \n",
    "                per_sample_grads = vmap(jacrev(func_lossg), (None, 0, 0))(func_params, Xg, Fg)\n",
    "                cnt = 0\n",
    "                for g in per_sample_grads: \n",
    "                    g = g.detach()\n",
    "                    J_g = g.reshape(len(g),-1) if cnt == 0 else torch.hstack([J_g,g.reshape(len(g),-1)])\n",
    "                    cnt = 1\n",
    "            \n",
    "                J = torch.cat( ( J_d/NL_sqrt[1] , J_b/NL_sqrt[2] , J_g/NL_sqrt[3] ) ).detach()\n",
    "                \n",
    "                ### info. normal equation of J\n",
    "                J_product = J.t()@J\n",
    "                rhs       = - J.t()@L\n",
    "        \n",
    "                    \n",
    "            with torch.no_grad():\n",
    "                ### solve the linear system\n",
    "                dp  = torch.linalg.solve( J_product + mu*I_pvec , rhs )\n",
    "                cnt = 0\n",
    "                for p in func_params:\n",
    "                    mm   = torch.Tensor([p.shape]).tolist()[0]\n",
    "                    num  = int( functools.reduce( lambda x,y : x*y, mm, 1 ) )\n",
    "                    p   += dp[cnt:cnt+num].reshape(p.shape)\n",
    "                    cnt += num\n",
    "            \n",
    "            ### Compute loss_new    \n",
    "            Ld = vmap((func_lossd), (None, 0, 0))(func_params, Xd, Fd).flatten().detach()\n",
    "            Lb = vmap((func_lossb), (None, 0, 0))(func_params, Xb, Fb).flatten().detach()\n",
    "            Lg = vmap((func_lossg), (None, 0, 0))(func_params, Xg, Fg).flatten().detach()\n",
    "            L  = torch.cat( ( Ld/NL_sqrt[1] , Lb/NL_sqrt[2] , Lg/NL_sqrt[3] ) )\n",
    "            L  = L.reshape( NL[0] , 1 ).detach()\n",
    "            loss_new = torch.sum(L*L).item()\n",
    "            lsd_sum = torch.sum(Ld*Ld)/NL[1]\n",
    "            lsb_sum = torch.sum(Lb*Lb)/NL[2]\n",
    "            lsg_sum = torch.sum(Lg*Lg)/NL[3]\n",
    "            loss_dbg_new = [lsd_sum.item(), lsb_sum.item(), lsg_sum.item()]\n",
    "    \n",
    "                \n",
    "            # strategy to update mu\n",
    "            if ( step > 0 ):\n",
    "                \n",
    "                with torch.no_grad():\n",
    "             \n",
    "                    # accept update \n",
    "                    if loss_new < loss_old:\n",
    "                        p_vec_old  = p_vec.detach()\n",
    "                        dp_old     = dp\n",
    "                        L_old      = L\n",
    "                        J_old      = J\n",
    "                        mu         = max( mu/mu_div , tol_machine )\n",
    "                        criterion  = True #False\n",
    "                        Comput_old = False\n",
    "                        lossval.append(loss_new)\n",
    "                        lossval_dbg.append(loss_dbg_new)\n",
    "                    \n",
    "                    else:\n",
    "                        cosine = nn.functional.cosine_similarity(dp, dp_old, dim=0, eps=1e-15)\n",
    "                        cosine_check = (1.-cosine)*loss_new > min(lossval) # loss_old\n",
    "                        if cosine_check: # give up the direction\n",
    "                            cnt=0\n",
    "                            for p in func_params:\n",
    "                                mm   = torch.Tensor([p.shape]).tolist()[0]\n",
    "                                num  = int( functools.reduce(lambda x,y: x*y, mm, 1) )\n",
    "                                p   -= dp[cnt:cnt+num].reshape(p.shape)\n",
    "                                cnt += num\n",
    "                            mu = min( mu_mul*mu , mu_max )\n",
    "                            criterion  = False\n",
    "                            Comput_old = False\n",
    "                        else: # accept \n",
    "                            p_vec_old = p_vec.detach()\n",
    "                            dp_old    = dp \n",
    "                            L_old     = L\n",
    "                            J_old     = J\n",
    "                            mu        = max( mu/mu_div , tol_machine )       \n",
    "                            criterion  = True\n",
    "                            Comput_old = False\n",
    "                        lossval.append(loss_old)\n",
    "                        lossval_dbg.append(loss_dbg_old)\n",
    "            \n",
    "            else:   # for old info. \n",
    "       \n",
    "                with torch.no_grad():\n",
    "              \n",
    "                    p_vec_old  = p_vec.detach()\n",
    "                    dp_old     = dp\n",
    "                    L_old      = L\n",
    "                    J_old      = J\n",
    "                    mu         = max( mu/mu_div , tol_machine )\n",
    "                    criterion  = True\n",
    "                    Comput_old = False\n",
    "                    lossval.append(loss_new)\n",
    "                    lossval_dbg.append(loss_dbg_new)\n",
    "            \n",
    "           \n",
    "            if step % ls_check == ls_check0:\n",
    "                print(\"Step %s: \" % (step) )\n",
    "                print(f\" training loss: {lossval[-1]:.4e}\")\n",
    "            \n",
    "            step += 1\n",
    "                  \n",
    "        print(\"Step %s: \" % (step-1) )\n",
    "        print(f\" training loss: {lossval[-1]:.4e}\")\n",
    "        print('finished')\n",
    "        lossval     = lossval[1:]\n",
    "        lossval_dbg = lossval_dbg[1:]\n",
    "        relerr_loss = lossval[-1]\n",
    "        return lossval, lossval_dbg, relerr_loss\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print('Interrupt')\n",
    "        print('steps = ', step)\n",
    "        lossval     = lossval[1:]\n",
    "        lossval_dbg = lossval_dbg[1:]\n",
    "        relerr_loss = lossval[-1]\n",
    "        return lossval, lossval_dbg, relerr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3a1f7be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training points in the bulk domain is 500\n",
      "No. of training points at the outer boundary is 1064\n",
      "No. of training points at the interface is 1064\n",
      "No. of overall training points is 2628\n",
      "No. of trainable parameters = 360\n",
      "Step 499: \n",
      " training loss: 6.1177e-08\n",
      "Step 999: \n",
      " training loss: 7.6182e-10\n",
      "Step 1499: \n",
      " training loss: 7.6182e-10\n",
      "Step 1999: \n",
      " training loss: 2.5419e-10\n",
      "Step 2499: \n",
      " training loss: 1.4325e-10\n",
      "Step 2999: \n",
      " training loss: 1.4130e-10\n",
      "Step 3000: \n",
      " training loss: 1.4130e-10\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# storages for errors, time instants, and IRK stages\n",
    "relerr_loss = []\n",
    "for char in char_id:\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # NN structure\n",
    "    if n_depth == 1:   # Shallow NN\n",
    "        model = NeuralNet_Shallow( n_input , n_hidden , n_output ).double().to(device)\n",
    "    else:   # Deep NN\n",
    "        model = NeuralNet_Deep( n_input , n_hidden , n_output , n_depth ).double().to(device)\n",
    "\n",
    "    # use Pytorch and functorch\n",
    "    func_model, func_params = make_functional(model)\n",
    "    \n",
    "    # generate training data\n",
    "    Xd_tr, Fd_tr, Xb_tr, Fb_tr, Xg_tr, Fg_tr, NL_tr, NL_sqrt_tr = generate_data(1, N_trd, N_trb, N_trg)\n",
    "    tr_input = DataInput(Xd=Xd_tr, Fd=Fd_tr, Xb=Xb_tr, Fb=Fb_tr, Xg=Xg_tr, Fg=Fg_tr, NL=NL_tr, NL_sqrt=NL_sqrt_tr)\n",
    "    \n",
    "    # initialization of LM\n",
    "    p_vec_old, dp_old, L_old, J_old = generate_initial_LM(func_params, NL_tr[1], NL_tr[2], NL_tr[3])\n",
    "    print(f\"No. of trainable parameters = {len(p_vec_old)}\")\n",
    "        \n",
    "    # LM_setup\n",
    "    mu = 10**(8)\n",
    "    criterion = True\n",
    "    LM_setup = LM_Setup( p_vec_o=p_vec_old , dp_o=dp_old , L_o=L_old , J_o=J_old , mu0=mu , criterion=criterion )\n",
    "\n",
    "    # allocate loss\n",
    "    lossval         = []\n",
    "    lossval_dbg     = []\n",
    "    lossval.append(1.)\n",
    "    lossval_dbg.append([1.,1.,1.])\n",
    "\n",
    "    # train the model by LM optimizer\n",
    "    lossval, lossval_dbg, relerr_loss_char = train_PINNs_LM(func_params, LM_setup, tr_input, lossval, lossval_dbg)\n",
    "    relerr_loss.append(relerr_loss_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30bc81d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot evolution of loss\n",
    "N_loss = len(lossval)\n",
    "lossval        = np.array(lossval).reshape(N_loss,1)\n",
    "epochcol = np.linspace(1, N_loss, N_loss).reshape(N_loss,1)\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "\n",
    "plt.semilogy(epochcol, lossval)\n",
    "plt.title('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe66702",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28868c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of testing points: \n",
      "    N_tsd = 50000\n",
      "    N_tsb = 106400\n",
      "    N_tsg = 106400\n",
      "    Overall: 262800\n",
      "No. of testing points at the outer boundary is 106400\n",
      "No. of testing points in the bulk domain is 50000\n",
      "No. of testing points at the interface is 106400\n"
     ]
    }
   ],
   "source": [
    "# No. of testing points\n",
    "print(f'No. of testing points: ')\n",
    "print(f'    N_tsd = {N_tsd_final}')\n",
    "print(f'    N_tsb = {N_tsb_final}')\n",
    "print(f'    N_tsg = {N_tsg_final}')\n",
    "print(f'    Overall: {N_tsd_final + N_tsb_final + N_tsg_final}')\n",
    "\n",
    "##### generate interior testing points and calculate error\n",
    "# X_tsb: boundary points\n",
    "X_tsb = lhs( dims , samples=N_tsb_final )\n",
    "X_tsb = spnorm( loc=0 , scale=1 ).ppf(X_tsb)\n",
    "X_tsb = Rad*X_tsb/np.expand_dims( np.linalg.norm(X_tsb,axis=1) , axis=1 )\n",
    "X_tsb = X_tsb.reshape( ( N_tsb_final , dims ) )\n",
    "qo, q = levfun(0, X_tsb)\n",
    "ref_u_tsb = exact_u(X_tsb, qo)\n",
    "X_tsb = np.hstack( ( X_tsb , q ) )\n",
    "print(f'No. of testing points at the outer boundary is {N_tsb_final}')\n",
    "\n",
    "# X_tsd: interior points \n",
    "r_tsd = lhs(1, samples=N_tsd_final)\n",
    "r_tsd = r_tsd**(1./dims)\n",
    "X_tsd = spnorm( loc=0 , scale=1 ).ppf( lhs(dims,samples=N_tsd_final) )\n",
    "X_tsd = Rad*X_tsd/np.expand_dims( np.linalg.norm(X_tsd,axis=1) , axis=1 )\n",
    "X_tsd = r_tsd*X_tsd\n",
    "X_tsd = X_tsd.reshape( (N_tsd_final,dims) )\n",
    "qo, q = levfun(0, X_tsd)\n",
    "ref_u_tsd = exact_u(X_tsd, qo)\n",
    "X_tsd = np.hstack( ( X_tsd , q ) )\n",
    "\n",
    "print(f'No. of testing points in the bulk domain is {N_tsd_final}')\n",
    "\n",
    "##### generate interfacial testing points and calculate error\n",
    "## X_gma: points on the interface\n",
    "X_tsg = lhs(dims, samples=N_tsg_final)\n",
    "X_tsg = spnorm( loc=0 , scale=1 ).ppf(X_tsg)\n",
    "X_tsg = rk*X_tsg/np.expand_dims( np.linalg.norm(X_tsg,axis=1) , axis=1 )\n",
    "X_tsg = X_tsg.reshape( (N_tsg_final,dims) )\n",
    "qo = np.zeros( (len(X_tsg[:,0]),1) )\n",
    "ref_u_tsg = exact_u(X_tsg, qo)\n",
    "X_tsg = np.hstack( ( X_tsg , qo ) )\n",
    "\n",
    "print(f'No. of testing points at the interface is {N_tsg_final}')\n",
    "\n",
    "## normal derivative jump condition for level function\n",
    "qdn_tsg = jump_lvdn(X_tsg)\n",
    "## normal derivative jump condition for u\n",
    "udn_tsg = jump_btadun(X_tsg) \n",
    "\n",
    "X_tst = torch.tensor( np.vstack( ( X_tsd , X_tsb , X_tsg ) ) ).double().to(device)\n",
    "ref_u_tst = np.vstack( ( ref_u_tsd , ref_u_tsb , ref_u_tsg ) )\n",
    "ref_u_tst_infnorm = np.linalg.norm( ref_u_tst , np.inf )\n",
    "ref_u_tst_2norm   = np.linalg.norm( ref_u_tst , 2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73da605a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_inf err.    : 1.4001e-05\n",
      "L_inf relerr. : 6.3280e-06\n",
      "L_2 err.    : 8.9873e-04\n",
      "L_2 relerr. : 1.6262e-06\n"
     ]
    }
   ],
   "source": [
    "pred_u = func_model(func_params, X_tst).cpu().detach().numpy().flatten()\n",
    "abserr = np.abs( pred_u - ref_u_tst.flatten() )\n",
    "err_inf = np.linalg.norm(abserr, np.inf)\n",
    "err_L2  = np.linalg.norm(abserr, 2)\n",
    "relerr_inf = err_inf / ref_u_tst_infnorm\n",
    "print(f\"L_inf err.    : {err_inf:.4e}\")\n",
    "print(f\"L_inf relerr. : {relerr_inf:.4e}\")\n",
    "relerr_L2  = err_L2 / ref_u_tst_2norm\n",
    "print(f\"L_2 err.    : {err_L2:.4e}\")\n",
    "print(f\"L_2 relerr. : {relerr_L2:.4e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
